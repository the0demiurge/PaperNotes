{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["+","15","2","2001.icann.learn","2001.reinforc","2003.natur","2015.emnlp.languag","2016.aaai.learn","2016.neural","2016.nips.learn","2016.openai.rl$2$","2017.nips.optim","2019.nature.grandmast","agent","architectur","base","blog:","book","build.sh","ci.","cname","cognition.md","control","deep","dependencies.md","descent","descent.md","directories,","download","epub","fast","few","file","foundat","game","gitbook,","github","gradient","header","ii","introduct","learn","learning.md","level","like","list","logic","long","lstm","markovian","mathmet","meta","mobi","model","multi","network","neural","non","note","notes,","offlin","onlin","pages,","papernot","papers.pythonic.lif","power","read","reading:","readme.md","reinforc","reviews.neur","robust","search","shot","size","slow","source:","starcraft","step","summary.md","task","term","text","this?","training.md","travi","typora.","understand","us","version","via","write","│","└──","├──","最好的电子笔记本：gitbook"],"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":["2001.icann.learn","descent","gradient","learn","us","·学习器：lstm","元学习器：rl","训练：当预测错了就惩罚，通过这种方式能让lstm收敛更快"],"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":["2001.reinforc","depend","learn","long","lstm","markovian","maze","non","t","task","term","使用强化学习与lstm结合，解决非马尔可夫问题或部分可观测马尔可夫决策过程（pomdps）","倒立摆","实验","评价","这篇文章可以用于在参考文献中提一下","这篇文章比较早（2001年发表），受限于当时的计算能力、调参技巧等，当时的实验都非常简单，没有太大参考意义；"],"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":["2003.natur",">视觉空间",">语言区","cognit","foundat","logic","mathmet","reviews.neur","代数","推理","由于现有的研究都很浅，所以没有得到更详细的结论。说明推理和语言有关，代数和视觉有关，这在深度网络设计的时候有一定的参考价值（大概吧）。","结论","这篇是讲逻辑与数学认知的生理学基础，使用脑部扫描对逻辑和数学区域进行一些粗浅的认识。"],"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":["2015.emnlp.languag","base","deep","dqn，跑文字冒险类游戏。","game","learn","reinforc","stack","text","understand","us","了一个","用一个游戏训练之后训练另一个游戏，能加快训练速度，实际上只有一个游戏的话语料也太少了吧，而且这个游戏的单词量也少得可怜；在一个文字冒险类游戏里面通关根本算不上是学会了什么或者能泛化之类的，因为完全靠记忆路径（文字冒险类游戏实际上是一个树结构的分支模型）就能通关了，通关了说明不了太多问题。","这篇文章没有什么意思，用lstm"],"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":["(x_i;","+","1}))r=−​s−1​​1​​​s=2​∑​s​​(log(e​s​​)−log(e​s−1​​))，其中s为总优化步数，回报实质上是衰减的误差变化量。","1}\\sum\\limits_{s=2}^s(log(e_s)","2016.aaai.learn","\\delta","\\frac{1}{","\\omega","\\omega)))log(var(δf(x​i​​;ω)))","\\omega)))log(var(δ​f​^​​(x​i​​;ω+δω)))","control","f","log(e_{","log(var(δf(xi;ω)))log(var(\\delta","log(var(δf^(xi;ω+δω)))log(var(\\delta\\hat","network","neural","rl的网络结构似乎只有一层","robust","size","step","tldr","train","为什么初始学习率差异很大（2个数量级），sgd与rmsprop收敛速度却差不多","为什么收敛曲线没有画出收敛到稳态的整个过程，最终的收敛结果才重要","使用了一种类似于trpo的强化学习算法：reps，限制新旧策略的kl散度。","使用了几个特征：","做法","可以引用这篇文章","回报设计：r=−1s−1∑s=2s(log(es)−log(es−1))r=","引用","收敛到的结果对学习率不敏感，而rmsprop与sgd都对初始学习率敏感","效果","疑点","目的：为了避免初始学习率依赖"],"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":["2016.neural","architectur","brain","cell更是使用已经人为确定好的结构，把易于替代的ops（比如add,","cell的生成）就用了400块gpu，在6e+16的搜索空间中迭代了15000回。按照每次迭代1小时算，耗费了2个gpu年（除以400个gpu后实际时间耗费不到2天）。使用这么大的运算量，就算用遗传算法大概也能得到相当好的结果，很难说能不能算是强化学习的功劳，因为按照我的经验来说，很可能随机初始化的时候初始解就处在了一个相当不错的位置，挑最好的那个网络来说事随机性太强。","elem_mut）当作参数让网络预测，无法用于可变结构生成，灵活性基本没有。","gool","learn","reinforc","reput","search","作品，因此","使用rnn生成一个序列，实际上方法很简单。按顺序生成cnn的参数组合、shortcut节点等参数，然后stack一下就能生成cnn；至于自动生成lstm","应该不错。","方法","评价","这个实验只有google有实力做，因为光第二个试验（lstm"],"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":["2016.nips.learn","adam","algorithm","connected训练的泛化不到convolut","dataset","deep","descent","experi","gradient","layer","learn","method","mind","mnist","nag","problem","quadrat","rmsprop","sgd","不错","作品，reput","使用aur度量算法性能","每维采取共享的策略","用full","用sigmoid训练的泛化不到relu","输入是梯度"],"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":["2016.openai.rl$2$","acknowledg","fast","learn","reinforc","rl","slow","tldr","via","库，可以进行参考。","本文值得参考的地方","这篇论文完全没有写出该方法的细节，只用了描述性的自然语言，所以无法进行复现，更难以进行评价。","里面使用了一个"],"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":["2017.nips.optim","cell","coordin","few","gradient，","learn","learner","meta","model","paramet","shot","state","state，将学习率对应到lstm的输入，将梯度对应到lstm的candid","tldr","使用了梯度，在不同参数数目下的问题之间难以进行迁移。后来使用了share","使用元数据集，即多组数据集","将模型参数对应到lstm的cell","解决的问题：few","问题"],"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":["10","1e26","1e3","1e4","1}a​1:t−1​​","2019.nature.grandmast",">",">exploit",">leagu",">main",">oppo","a1:t−1a_{1:","act","action","actor","advantag","agent","agent'","agent:","agents:","algorithm(upgo)","algorithm:","alpahstar","apm","asynchron","attent","attr","auto","both","build","camera","challeng","choic","clip","combin","combinatori","comput","connections:","counter","critic","current","cyclic,","data","differ","disobey","effect","env","episod","estim","experi","exploit","explor","exploration.","featur","fictiti","find","fix","follow","function","good","gradient","hard","hardli","human","ii","imit","imperfect","import","inform","init","integr","issu","item","iter","larg","latenc","layer","leagu","learn","learning(td(λ\\lambdaλ))","level","limit","list","locat","lstm:","main","map","map(s,","min","model","monitor","move","move,","multi","naiv","network","network:","next","non","novel","o1:to_{1:","ob","object:","observ","oppo","oppon","opponents'","p","partial","perspect","pfsp","play","play(pfsp)","play,","player'","pointer","polici","policies.","priortiz","punish","re","real","realli","recurr","regress","reinforc","repali","reward","sampling(v","scatter","select","self","share","similar","space","space(1e3)","spacial:","sparse.","spatial","starcraft","state","statist","step","strategi","strategy,","structur","subset","superv","system","t","tactic","target","target,","tempor","time","time:","to,","trace)","train","transit","type(1e2):","t}o​1:t​​","unit","us","valu","vast","view","weak","weights:","within","without","worker","world","z)","zzz,","πθ(at∣st,z)\\pi_\\theta(a_t|s_t,z)π​θ​​(a​t​​∣s​t​​,z)"]},"length":11},"tokenStore":{"root":{"1":{"0":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"5":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}},"docs":{},"}":{"docs":{},")":{"docs":{},")":{"docs":{},"r":{"docs":{},"=":{"docs":{},"−":{"docs":{},"​":{"docs":{},"s":{"docs":{},"−":{"1":{"docs":{},"​":{"docs":{},"​":{"1":{"docs":{},"​":{"docs":{},"​":{"docs":{},"​":{"docs":{},"s":{"docs":{},"=":{"2":{"docs":{},"​":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"s":{"docs":{},"​":{"docs":{},"​":{"docs":{},"(":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"e":{"docs":{},"​":{"docs":{},"s":{"docs":{},"​":{"docs":{},"​":{"docs":{},")":{"docs":{},"−":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"e":{"docs":{},"​":{"docs":{},"s":{"docs":{},"−":{"1":{"docs":{},"​":{"docs":{},"​":{"docs":{},")":{"docs":{},")":{"docs":{},"，":{"docs":{},"其":{"docs":{},"中":{"docs":{},"s":{"docs":{},"为":{"docs":{},"总":{"docs":{},"优":{"docs":{},"化":{"docs":{},"步":{"docs":{},"数":{"docs":{},"，":{"docs":{},"回":{"docs":{},"报":{"docs":{},"实":{"docs":{},"质":{"docs":{},"上":{"docs":{},"是":{"docs":{},"衰":{"docs":{},"减":{"docs":{},"的":{"docs":{},"误":{"docs":{},"差":{"docs":{},"变":{"docs":{},"化":{"docs":{},"量":{"docs":{},"。":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}}},"docs":{}}}}}}}}},"\\":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"\\":{"docs":{},"l":{"docs":{},"i":{"docs":{},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},"_":{"docs":{},"{":{"docs":{},"s":{"docs":{},"=":{"2":{"docs":{},"}":{"docs":{},"^":{"docs":{},"s":{"docs":{},"(":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"e":{"docs":{},"_":{"docs":{},"s":{"docs":{},")":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}},"a":{"docs":{},"​":{"1":{"docs":{},":":{"docs":{},"t":{"docs":{},"−":{"1":{"docs":{},"​":{"docs":{},"​":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"docs":{}}}}},"docs":{}}}},"e":{"2":{"6":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"docs":{}},"3":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"4":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"docs":{}}},"2":{"0":{"0":{"1":{"docs":{},".":{"docs":{},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"n":{"docs":{},"n":{"docs":{},".":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":2}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}}}}}}}}}}}},"3":{"docs":{},".":{"docs":{},"n":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":1.6666666666666665}}}}}}}}},"docs":{}},"1":{"5":{"docs":{},".":{"docs":{},"e":{"docs":{},"m":{"docs":{},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},".":{"docs":{},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{},"a":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112}}}}}}}}}}}}}}}}},"6":{"docs":{},".":{"docs":{},"a":{"docs":{},"a":{"docs":{},"a":{"docs":{},"i":{"docs":{},".":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25}}}}}}}}}}}},"n":{"docs":{},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":2}}}}}}},"i":{"docs":{},"p":{"docs":{},"s":{"docs":{},".":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":1.6666666666666665}}}}}}}}}}}},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"i":{"docs":{},".":{"docs":{},"r":{"docs":{},"l":{"docs":{},"$":{"2":{"docs":{},"$":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":1.25}}}},"docs":{}}}}}}}}}}}}},"7":{"docs":{},".":{"docs":{},"n":{"docs":{},"i":{"docs":{},"p":{"docs":{},"s":{"docs":{},".":{"docs":{},"o":{"docs":{},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":2}}}}}}}}}}}}}},"9":{"docs":{},".":{"docs":{},"n":{"docs":{},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},".":{"docs":{},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1111111111111112}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}},"docs":{},"+":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}},"a":{"1":{"docs":{},":":{"docs":{},"t":{"docs":{},"−":{"1":{"docs":{},"a":{"docs":{},"_":{"docs":{},"{":{"1":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"docs":{}}}}},"docs":{}}}}},"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.135304659498208}},"'":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"s":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":2}}}}}}}}}}}},"d":{"docs":{},"a":{"docs":{},"m":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}},"v":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{},"a":{"docs":{},"g":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}},"l":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}},"(":{"docs":{},"u":{"docs":{},"p":{"docs":{},"g":{"docs":{},"o":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}},"p":{"docs":{},"a":{"docs":{},"h":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}},"c":{"docs":{},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285}}}}}}}}}},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.028225806451612902}}}}},"o":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"p":{"docs":{},"m":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"s":{"docs":{},"y":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{},"r":{"docs":{},"o":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112}}}}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},":":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}},"t":{"docs":{},"h":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"u":{"docs":{},"i":{"docs":{},"l":{"docs":{},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},".":{"docs":{},"s":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}},"c":{"docs":{},"i":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"o":{"docs":{},"g":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":1.6666666666666665}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}}}},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25}}}}}},"n":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"不":{"docs":{},"到":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"v":{"docs":{},"o":{"docs":{},"l":{"docs":{},"u":{"docs":{},"t":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}},"m":{"docs":{},"b":{"docs":{},"i":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"a":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}},"更":{"docs":{},"是":{"docs":{},"使":{"docs":{},"用":{"docs":{},"已":{"docs":{},"经":{"docs":{},"人":{"docs":{},"为":{"docs":{},"确":{"docs":{},"定":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"构":{"docs":{},"，":{"docs":{},"把":{"docs":{},"易":{"docs":{},"于":{"docs":{},"替":{"docs":{},"代":{"docs":{},"的":{"docs":{},"o":{"docs":{},"p":{"docs":{},"s":{"docs":{},"（":{"docs":{},"比":{"docs":{},"如":{"docs":{},"a":{"docs":{},"d":{"docs":{},"d":{"docs":{},",":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"的":{"docs":{},"生":{"docs":{},"成":{"docs":{},"）":{"docs":{},"就":{"docs":{},"用":{"docs":{},"了":{"4":{"0":{"0":{"docs":{},"块":{"docs":{},"g":{"docs":{},"p":{"docs":{},"u":{"docs":{},"，":{"docs":{},"在":{"6":{"docs":{},"e":{"docs":{},"+":{"1":{"6":{"docs":{},"的":{"docs":{},"搜":{"docs":{},"索":{"docs":{},"空":{"docs":{},"间":{"docs":{},"中":{"docs":{},"迭":{"docs":{},"代":{"docs":{},"了":{"1":{"5":{"0":{"0":{"0":{"docs":{},"回":{"docs":{},"。":{"docs":{},"按":{"docs":{},"照":{"docs":{},"每":{"docs":{},"次":{"docs":{},"迭":{"docs":{},"代":{"1":{"docs":{},"小":{"docs":{},"时":{"docs":{},"算":{"docs":{},"，":{"docs":{},"耗":{"docs":{},"费":{"docs":{},"了":{"2":{"docs":{},"个":{"docs":{},"g":{"docs":{},"p":{"docs":{},"u":{"docs":{},"年":{"docs":{},"（":{"docs":{},"除":{"docs":{},"以":{"4":{"0":{"0":{"docs":{},"个":{"docs":{},"g":{"docs":{},"p":{"docs":{},"u":{"docs":{},"后":{"docs":{},"实":{"docs":{},"际":{"docs":{},"时":{"docs":{},"间":{"docs":{},"耗":{"docs":{},"费":{"docs":{},"不":{"docs":{},"到":{"2":{"docs":{},"天":{"docs":{},"）":{"docs":{},"。":{"docs":{},"使":{"docs":{},"用":{"docs":{},"这":{"docs":{},"么":{"docs":{},"大":{"docs":{},"的":{"docs":{},"运":{"docs":{},"算":{"docs":{},"量":{"docs":{},"，":{"docs":{},"就":{"docs":{},"算":{"docs":{},"用":{"docs":{},"遗":{"docs":{},"传":{"docs":{},"算":{"docs":{},"法":{"docs":{},"大":{"docs":{},"概":{"docs":{},"也":{"docs":{},"能":{"docs":{},"得":{"docs":{},"到":{"docs":{},"相":{"docs":{},"当":{"docs":{},"好":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"，":{"docs":{},"很":{"docs":{},"难":{"docs":{},"说":{"docs":{},"能":{"docs":{},"不":{"docs":{},"能":{"docs":{},"算":{"docs":{},"是":{"docs":{},"强":{"docs":{},"化":{"docs":{},"学":{"docs":{},"习":{"docs":{},"的":{"docs":{},"功":{"docs":{},"劳":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"按":{"docs":{},"照":{"docs":{},"我":{"docs":{},"的":{"docs":{},"经":{"docs":{},"验":{"docs":{},"来":{"docs":{},"说":{"docs":{},"，":{"docs":{},"很":{"docs":{},"可":{"docs":{},"能":{"docs":{},"随":{"docs":{},"机":{"docs":{},"初":{"docs":{},"始":{"docs":{},"化":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"初":{"docs":{},"始":{"docs":{},"解":{"docs":{},"就":{"docs":{},"处":{"docs":{},"在":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{},"相":{"docs":{},"当":{"docs":{},"不":{"docs":{},"错":{"docs":{},"的":{"docs":{},"位":{"docs":{},"置":{"docs":{},"，":{"docs":{},"挑":{"docs":{},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"那":{"docs":{},"个":{"docs":{},"网":{"docs":{},"络":{"docs":{},"来":{"docs":{},"说":{"docs":{},"事":{"docs":{},"随":{"docs":{},"机":{"docs":{},"性":{"docs":{},"太":{"docs":{},"强":{"docs":{},"。":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"docs":{}},"docs":{}}}},"docs":{}}}}}}}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}},"h":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"g":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"o":{"docs":{},"i":{"docs":{},"c":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"l":{"docs":{},"i":{"docs":{},"p":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}}},"y":{"docs":{},"c":{"docs":{},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"e":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112},"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"i":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}}}}}}},"s":{"docs":{},"c":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":2},"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":3.333333333333333}},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.014184397163120567}}}}}}}}}}},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"s":{"docs":{},"o":{"docs":{},"b":{"docs":{},"e":{"docs":{},"y":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}},"q":{"docs":{},"n":{"docs":{},"，":{"docs":{},"跑":{"docs":{},"文":{"docs":{},"字":{"docs":{},"冒":{"docs":{},"险":{"docs":{},"类":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"。":{"docs":{"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":0.2}}}}}}}}}}}}}},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}},"e":{"docs":{},"p":{"docs":{},"u":{"docs":{},"b":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}},"i":{"docs":{},"s":{"docs":{},"o":{"docs":{},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{},"_":{"docs":{},"m":{"docs":{},"u":{"docs":{},"t":{"docs":{},"）":{"docs":{},"当":{"docs":{},"作":{"docs":{},"参":{"docs":{},"数":{"docs":{},"让":{"docs":{},"网":{"docs":{},"络":{"docs":{},"预":{"docs":{},"测":{"docs":{},"，":{"docs":{},"无":{"docs":{},"法":{"docs":{},"用":{"docs":{},"于":{"docs":{},"可":{"docs":{},"变":{"docs":{},"结":{"docs":{},"构":{"docs":{},"生":{"docs":{},"成":{"docs":{},"，":{"docs":{},"灵":{"docs":{},"活":{"docs":{},"性":{"docs":{},"基":{"docs":{},"本":{"docs":{},"没":{"docs":{},"有":{"docs":{},"。":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"l":{"docs":{},"o":{"docs":{},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.03225806451612903}}}},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"n":{"docs":{},"v":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"f":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.06896551724137931}},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":1.25}}}}},"e":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":2}}},"a":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"i":{"docs":{},"l":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.014184397163120567}}}},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}},"n":{"docs":{},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}},"x":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":1.6666666666666665}}}}}}},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112}}}}},"i":{"docs":{},"t":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.02127659574468085},"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":2},"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":3.333333333333333},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"，":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}}}}},"o":{"docs":{},"o":{"docs":{},"l":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"h":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"l":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"u":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.012096774193548387}}}}}}},"i":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1111111111111112}}},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":10}}}}}}}},"e":{"docs":{},"g":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"s":{"docs":{},"s":{"docs":{},"u":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0425531914893617},"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":2},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":2},"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":1.6666666666666665},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":2.5},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":2.1176470588235294},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1151433691756274}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.03546099290780142}}}}},"(":{"docs":{},"t":{"docs":{},"d":{"docs":{},"(":{"docs":{},"λ":{"docs":{},"\\":{"docs":{},"l":{"docs":{},"a":{"docs":{},"m":{"docs":{},"b":{"docs":{},"d":{"docs":{},"a":{"docs":{},"λ":{"docs":{},")":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}},"g":{"docs":{},"u":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.024193548387096774}}}}},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1111111111111112}}}}}},"i":{"docs":{},"k":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"o":{"docs":{},"g":{"docs":{},"i":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":1.6666666666666665}}}},"(":{"docs":{},"e":{"docs":{},"_":{"docs":{},"{":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"(":{"docs":{},"δ":{"docs":{},"f":{"docs":{},"(":{"docs":{},"x":{"docs":{},"i":{"docs":{},";":{"docs":{},"ω":{"docs":{},")":{"docs":{},")":{"docs":{},")":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"(":{"docs":{},"\\":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"t":{"docs":{},"a":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}},"^":{"docs":{},"(":{"docs":{},"x":{"docs":{},"i":{"docs":{},";":{"docs":{},"ω":{"docs":{},"+":{"docs":{},"δ":{"docs":{},"ω":{"docs":{},")":{"docs":{},")":{"docs":{},")":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"(":{"docs":{},"\\":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"t":{"docs":{},"a":{"docs":{},"\\":{"docs":{},"h":{"docs":{},"a":{"docs":{},"t":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}}}},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"a":{"docs":{},"y":{"docs":{},"e":{"docs":{},"r":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"r":{"docs":{},"g":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"k":{"docs":{},"o":{"docs":{},"v":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":1.6666666666666665}}}}}}},"z":{"docs":{},"e":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125}}}},"i":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.04435483870967742}}}},"p":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}},"o":{"docs":{},"b":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":2},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"v":{"docs":{},"e":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1111111111111112}}}}}},"i":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"d":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25}}}}}},"x":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}},"t":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.014184397163120567}},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"a":{"docs":{},"g":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}},"i":{"docs":{},"v":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"o":{"1":{"docs":{},":":{"docs":{},"t":{"docs":{},"o":{"docs":{},"_":{"docs":{},"{":{"1":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"docs":{}}}}}}},"docs":{},"f":{"docs":{},"f":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}},"n":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"b":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.012096774193548387}},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"v":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"p":{"docs":{},"p":{"docs":{},"o":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.016129032258064516}},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},"'":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}}}},"p":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{},"o":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.014184397163120567}}}}},"s":{"docs":{},".":{"docs":{},"p":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{},"i":{"docs":{},"c":{"docs":{},".":{"docs":{},"l":{"docs":{},"i":{"docs":{},"f":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"o":{"docs":{},"w":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.016129032258064516}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}},"r":{"docs":{},"o":{"docs":{},"b":{"docs":{},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}},"i":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"z":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}}}},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}},"f":{"docs":{},"s":{"docs":{},"p":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"l":{"docs":{},"a":{"docs":{},"y":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.012096774193548387}},"(":{"docs":{},"p":{"docs":{},"f":{"docs":{},"s":{"docs":{},"p":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"e":{"docs":{},"r":{"docs":{},"'":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"h":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"r":{"docs":{},"e":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}},"a":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"m":{"docs":{},"e":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}},"l":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"l":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.0425531914893617},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":2},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":2.5},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1111111111111112}}}}}}}},"v":{"docs":{},"i":{"docs":{},"e":{"docs":{},"w":{"docs":{},"s":{"docs":{},".":{"docs":{},"n":{"docs":{},"e":{"docs":{},"u":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":1.6666666666666665}}}}}}}}}}}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"o":{"docs":{},"b":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25}}}}}}},"l":{"docs":{"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285}},"的":{"docs":{},"网":{"docs":{},"络":{"docs":{},"结":{"docs":{},"构":{"docs":{},"似":{"docs":{},"乎":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"层":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}},"m":{"docs":{},"s":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":2}}}}}},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"f":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.020161290322580645}}}}},"h":{"docs":{},"o":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":2.0588235294117645}}}},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25}}}},"m":{"docs":{},"i":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":1.25}}}}},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{},"e":{"docs":{},":":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"r":{"docs":{},"a":{"docs":{},"f":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1111111111111112}}}}}}}},"c":{"docs":{},"k":{"docs":{"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":0.2}}}},"t":{"docs":{},"e":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"，":{"docs":{},"将":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"对":{"docs":{},"应":{"docs":{},"到":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"的":{"docs":{},"输":{"docs":{},"入":{"docs":{},"，":{"docs":{},"将":{"docs":{},"梯":{"docs":{},"度":{"docs":{},"对":{"docs":{},"应":{"docs":{},"到":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"的":{"docs":{},"c":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"i":{"docs":{},"d":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"e":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.016129032258064516}}},"y":{"docs":{},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}}}},"u":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}},"b":{"docs":{},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"v":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}},"g":{"docs":{},"d":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"(":{"docs":{},"v":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}}},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}},"(":{"1":{"docs":{},"e":{"3":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"docs":{}}},"docs":{}}},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"r":{"docs":{},"s":{"docs":{},"e":{"docs":{},".":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"t":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}}}}},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"t":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}}}},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":1.1111111111111112}}}},"x":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112}}}},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"h":{"docs":{},"i":{"docs":{},"s":{"docs":{},"?":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":1.25},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}},"v":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}},"c":{"docs":{},"e":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"y":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"a":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"e":{"docs":{},"(":{"1":{"docs":{},"e":{"2":{"docs":{},")":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"docs":{}}},"docs":{}}}}},"l":{"docs":{},"d":{"docs":{},"r":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"o":{"docs":{},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"}":{"docs":{},"o":{"docs":{},"​":{"1":{"docs":{},":":{"docs":{},"t":{"docs":{},"​":{"docs":{},"​":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"docs":{}}}}},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112}}}}}}}}}},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}},"s":{"docs":{"./":{"ref":"./","tf":0.02127659574468085},"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":2},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":1.1111111111111112},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":1.1191756272401434}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}},"i":{"docs":{},"a":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":1.25}}},"e":{"docs":{},"w":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"s":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"w":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}},"e":{"docs":{},"a":{"docs":{},"k":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.008064516129032258}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"s":{"docs":{},":":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}},"l":{"docs":{},"d":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"│":{"docs":{"./":{"ref":"./","tf":0.07092198581560284}}},"└":{"docs":{},"─":{"docs":{},"─":{"docs":{"./":{"ref":"./","tf":0.02127659574468085}}}}},"├":{"docs":{},"─":{"docs":{},"─":{"docs":{"./":{"ref":"./","tf":0.09929078014184398}}}}},"最":{"docs":{},"好":{"docs":{},"的":{"docs":{},"电":{"docs":{},"子":{"docs":{},"笔":{"docs":{},"记":{"docs":{},"本":{"docs":{},"：":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.0070921985815602835}}}}}}}}}}}}}}}}}},"·":{"docs":{},"学":{"docs":{},"习":{"docs":{},"器":{"docs":{},"：":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":0.3333333333333333}}}}}}}}}}},"元":{"docs":{},"学":{"docs":{},"习":{"docs":{},"器":{"docs":{},"：":{"docs":{},"r":{"docs":{},"l":{"docs":{"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":0.3333333333333333}}}}}}}}},"训":{"docs":{},"练":{"docs":{},"：":{"docs":{},"当":{"docs":{},"预":{"docs":{},"测":{"docs":{},"错":{"docs":{},"了":{"docs":{},"就":{"docs":{},"惩":{"docs":{},"罚":{"docs":{},"，":{"docs":{},"通":{"docs":{},"过":{"docs":{},"这":{"docs":{},"种":{"docs":{},"方":{"docs":{},"式":{"docs":{},"能":{"docs":{},"让":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"收":{"docs":{},"敛":{"docs":{},"更":{"docs":{},"快":{"docs":{"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"ref":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","tf":0.3333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"使":{"docs":{},"用":{"docs":{},"强":{"docs":{},"化":{"docs":{},"学":{"docs":{},"习":{"docs":{},"与":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"结":{"docs":{},"合":{"docs":{},"，":{"docs":{},"解":{"docs":{},"决":{"docs":{},"非":{"docs":{},"马":{"docs":{},"尔":{"docs":{},"可":{"docs":{},"夫":{"docs":{},"问":{"docs":{},"题":{"docs":{},"或":{"docs":{},"部":{"docs":{},"分":{"docs":{},"可":{"docs":{},"观":{"docs":{},"测":{"docs":{},"马":{"docs":{},"尔":{"docs":{},"可":{"docs":{},"夫":{"docs":{},"决":{"docs":{},"策":{"docs":{},"过":{"docs":{},"程":{"docs":{},"（":{"docs":{},"p":{"docs":{},"o":{"docs":{},"m":{"docs":{},"d":{"docs":{},"p":{"docs":{},"s":{"docs":{},"）":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"了":{"docs":{},"一":{"docs":{},"种":{"docs":{},"类":{"docs":{},"似":{"docs":{},"于":{"docs":{},"t":{"docs":{},"r":{"docs":{},"p":{"docs":{},"o":{"docs":{},"的":{"docs":{},"强":{"docs":{},"化":{"docs":{},"学":{"docs":{},"习":{"docs":{},"算":{"docs":{},"法":{"docs":{},"：":{"docs":{},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"s":{"docs":{},"，":{"docs":{},"限":{"docs":{},"制":{"docs":{},"新":{"docs":{},"旧":{"docs":{},"策":{"docs":{},"略":{"docs":{},"的":{"docs":{},"k":{"docs":{},"l":{"docs":{},"散":{"docs":{},"度":{"docs":{},"。":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"几":{"docs":{},"个":{"docs":{},"特":{"docs":{},"征":{"docs":{},"：":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}},"梯":{"docs":{},"度":{"docs":{},"，":{"docs":{},"在":{"docs":{},"不":{"docs":{},"同":{"docs":{},"参":{"docs":{},"数":{"docs":{},"数":{"docs":{},"目":{"docs":{},"下":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"之":{"docs":{},"间":{"docs":{},"难":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"迁":{"docs":{},"移":{"docs":{},"。":{"docs":{},"后":{"docs":{},"来":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"s":{"docs":{},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"n":{"docs":{},"n":{"docs":{},"生":{"docs":{},"成":{"docs":{},"一":{"docs":{},"个":{"docs":{},"序":{"docs":{},"列":{"docs":{},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"方":{"docs":{},"法":{"docs":{},"很":{"docs":{},"简":{"docs":{},"单":{"docs":{},"。":{"docs":{},"按":{"docs":{},"顺":{"docs":{},"序":{"docs":{},"生":{"docs":{},"成":{"docs":{},"c":{"docs":{},"n":{"docs":{},"n":{"docs":{},"的":{"docs":{},"参":{"docs":{},"数":{"docs":{},"组":{"docs":{},"合":{"docs":{},"、":{"docs":{},"s":{"docs":{},"h":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"c":{"docs":{},"u":{"docs":{},"t":{"docs":{},"节":{"docs":{},"点":{"docs":{},"等":{"docs":{},"参":{"docs":{},"数":{"docs":{},"，":{"docs":{},"然":{"docs":{},"后":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{},"一":{"docs":{},"下":{"docs":{},"就":{"docs":{},"能":{"docs":{},"生":{"docs":{},"成":{"docs":{},"c":{"docs":{},"n":{"docs":{},"n":{"docs":{},"；":{"docs":{},"至":{"docs":{},"于":{"docs":{},"自":{"docs":{},"动":{"docs":{},"生":{"docs":{},"成":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"u":{"docs":{},"r":{"docs":{},"度":{"docs":{},"量":{"docs":{},"算":{"docs":{},"法":{"docs":{},"性":{"docs":{},"能":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}}}},"元":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{},"，":{"docs":{},"即":{"docs":{},"多":{"docs":{},"组":{"docs":{},"数":{"docs":{},"据":{"docs":{},"集":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}}}}}}}}}},"倒":{"docs":{},"立":{"docs":{},"摆":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125}}}}},"实":{"docs":{},"验":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125}}}},"评":{"docs":{},"价":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}},"这":{"docs":{},"篇":{"docs":{},"文":{"docs":{},"章":{"docs":{},"可":{"docs":{},"以":{"docs":{},"用":{"docs":{},"于":{"docs":{},"在":{"docs":{},"参":{"docs":{},"考":{"docs":{},"文":{"docs":{},"献":{"docs":{},"中":{"docs":{},"提":{"docs":{},"一":{"docs":{},"下":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125}}}}}}}}}}}}}}},"比":{"docs":{},"较":{"docs":{},"早":{"docs":{},"（":{"2":{"0":{"0":{"1":{"docs":{},"年":{"docs":{},"发":{"docs":{},"表":{"docs":{},"）":{"docs":{},"，":{"docs":{},"受":{"docs":{},"限":{"docs":{},"于":{"docs":{},"当":{"docs":{},"时":{"docs":{},"的":{"docs":{},"计":{"docs":{},"算":{"docs":{},"能":{"docs":{},"力":{"docs":{},"、":{"docs":{},"调":{"docs":{},"参":{"docs":{},"技":{"docs":{},"巧":{"docs":{},"等":{"docs":{},"，":{"docs":{},"当":{"docs":{},"时":{"docs":{},"的":{"docs":{},"实":{"docs":{},"验":{"docs":{},"都":{"docs":{},"非":{"docs":{},"常":{"docs":{},"简":{"docs":{},"单":{"docs":{},"，":{"docs":{},"没":{"docs":{},"有":{"docs":{},"太":{"docs":{},"大":{"docs":{},"参":{"docs":{},"考":{"docs":{},"意":{"docs":{},"义":{"docs":{},"；":{"docs":{"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"ref":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","tf":0.125}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}},"没":{"docs":{},"有":{"docs":{},"什":{"docs":{},"么":{"docs":{},"意":{"docs":{},"思":{"docs":{},"，":{"docs":{},"用":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":0.2}}}}}}}}}}}}}}}},"是":{"docs":{},"讲":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"与":{"docs":{},"数":{"docs":{},"学":{"docs":{},"认":{"docs":{},"知":{"docs":{},"的":{"docs":{},"生":{"docs":{},"理":{"docs":{},"学":{"docs":{},"基":{"docs":{},"础":{"docs":{},"，":{"docs":{},"使":{"docs":{},"用":{"docs":{},"脑":{"docs":{},"部":{"docs":{},"扫":{"docs":{},"描":{"docs":{},"对":{"docs":{},"逻":{"docs":{},"辑":{"docs":{},"和":{"docs":{},"数":{"docs":{},"学":{"docs":{},"区":{"docs":{},"域":{"docs":{},"进":{"docs":{},"行":{"docs":{},"一":{"docs":{},"些":{"docs":{},"粗":{"docs":{},"浅":{"docs":{},"的":{"docs":{},"认":{"docs":{},"识":{"docs":{},"。":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"论":{"docs":{},"文":{"docs":{},"完":{"docs":{},"全":{"docs":{},"没":{"docs":{},"有":{"docs":{},"写":{"docs":{},"出":{"docs":{},"该":{"docs":{},"方":{"docs":{},"法":{"docs":{},"的":{"docs":{},"细":{"docs":{},"节":{"docs":{},"，":{"docs":{},"只":{"docs":{},"用":{"docs":{},"了":{"docs":{},"描":{"docs":{},"述":{"docs":{},"性":{"docs":{},"的":{"docs":{},"自":{"docs":{},"然":{"docs":{},"语":{"docs":{},"言":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"无":{"docs":{},"法":{"docs":{},"进":{"docs":{},"行":{"docs":{},"复":{"docs":{},"现":{"docs":{},"，":{"docs":{},"更":{"docs":{},"难":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"评":{"docs":{},"价":{"docs":{},"。":{"docs":{"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"个":{"docs":{},"实":{"docs":{},"验":{"docs":{},"只":{"docs":{},"有":{"docs":{},"g":{"docs":{},"o":{"docs":{},"o":{"docs":{},"g":{"docs":{},"l":{"docs":{},"e":{"docs":{},"有":{"docs":{},"实":{"docs":{},"力":{"docs":{},"做":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"光":{"docs":{},"第":{"docs":{},"二":{"docs":{},"个":{"docs":{},"试":{"docs":{},"验":{"docs":{},"（":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},">":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.028225806451612902}},"视":{"docs":{},"觉":{"docs":{},"空":{"docs":{},"间":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}}}},"语":{"docs":{},"言":{"docs":{},"区":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}}},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"l":{"docs":{},"o":{"docs":{},"i":{"docs":{},"t":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"g":{"docs":{},"u":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.012096774193548387}}}}}},"o":{"docs":{},"p":{"docs":{},"p":{"docs":{},"o":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}},"代":{"docs":{},"数":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}},"推":{"docs":{},"理":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}},"由":{"docs":{},"于":{"docs":{},"现":{"docs":{},"有":{"docs":{},"的":{"docs":{},"研":{"docs":{},"究":{"docs":{},"都":{"docs":{},"很":{"docs":{},"浅":{"docs":{},"，":{"docs":{},"所":{"docs":{},"以":{"docs":{},"没":{"docs":{},"有":{"docs":{},"得":{"docs":{},"到":{"docs":{},"更":{"docs":{},"详":{"docs":{},"细":{"docs":{},"的":{"docs":{},"结":{"docs":{},"论":{"docs":{},"。":{"docs":{},"说":{"docs":{},"明":{"docs":{},"推":{"docs":{},"理":{"docs":{},"和":{"docs":{},"语":{"docs":{},"言":{"docs":{},"有":{"docs":{},"关":{"docs":{},"，":{"docs":{},"代":{"docs":{},"数":{"docs":{},"和":{"docs":{},"视":{"docs":{},"觉":{"docs":{},"有":{"docs":{},"关":{"docs":{},"，":{"docs":{},"这":{"docs":{},"在":{"docs":{},"深":{"docs":{},"度":{"docs":{},"网":{"docs":{},"络":{"docs":{},"设":{"docs":{},"计":{"docs":{},"的":{"docs":{},"时":{"docs":{},"候":{"docs":{},"有":{"docs":{},"一":{"docs":{},"定":{"docs":{},"的":{"docs":{},"参":{"docs":{},"考":{"docs":{},"价":{"docs":{},"值":{"docs":{},"（":{"docs":{},"大":{"docs":{},"概":{"docs":{},"吧":{"docs":{},"）":{"docs":{},"。":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"结":{"docs":{},"论":{"docs":{"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"ref":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","tf":0.14285714285714285}}}},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":0.2}}}}},"用":{"docs":{},"一":{"docs":{},"个":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"训":{"docs":{},"练":{"docs":{},"之":{"docs":{},"后":{"docs":{},"训":{"docs":{},"练":{"docs":{},"另":{"docs":{},"一":{"docs":{},"个":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"，":{"docs":{},"能":{"docs":{},"加":{"docs":{},"快":{"docs":{},"训":{"docs":{},"练":{"docs":{},"速":{"docs":{},"度":{"docs":{},"，":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"只":{"docs":{},"有":{"docs":{},"一":{"docs":{},"个":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"的":{"docs":{},"话":{"docs":{},"语":{"docs":{},"料":{"docs":{},"也":{"docs":{},"太":{"docs":{},"少":{"docs":{},"了":{"docs":{},"吧":{"docs":{},"，":{"docs":{},"而":{"docs":{},"且":{"docs":{},"这":{"docs":{},"个":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"的":{"docs":{},"单":{"docs":{},"词":{"docs":{},"量":{"docs":{},"也":{"docs":{},"少":{"docs":{},"得":{"docs":{},"可":{"docs":{},"怜":{"docs":{},"；":{"docs":{},"在":{"docs":{},"一":{"docs":{},"个":{"docs":{},"文":{"docs":{},"字":{"docs":{},"冒":{"docs":{},"险":{"docs":{},"类":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"里":{"docs":{},"面":{"docs":{},"通":{"docs":{},"关":{"docs":{},"根":{"docs":{},"本":{"docs":{},"算":{"docs":{},"不":{"docs":{},"上":{"docs":{},"是":{"docs":{},"学":{"docs":{},"会":{"docs":{},"了":{"docs":{},"什":{"docs":{},"么":{"docs":{},"或":{"docs":{},"者":{"docs":{},"能":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"之":{"docs":{},"类":{"docs":{},"的":{"docs":{},"，":{"docs":{},"因":{"docs":{},"为":{"docs":{},"完":{"docs":{},"全":{"docs":{},"靠":{"docs":{},"记":{"docs":{},"忆":{"docs":{},"路":{"docs":{},"径":{"docs":{},"（":{"docs":{},"文":{"docs":{},"字":{"docs":{},"冒":{"docs":{},"险":{"docs":{},"类":{"docs":{},"游":{"docs":{},"戏":{"docs":{},"实":{"docs":{},"际":{"docs":{},"上":{"docs":{},"是":{"docs":{},"一":{"docs":{},"个":{"docs":{},"树":{"docs":{},"结":{"docs":{},"构":{"docs":{},"的":{"docs":{},"分":{"docs":{},"支":{"docs":{},"模":{"docs":{},"型":{"docs":{},"）":{"docs":{},"就":{"docs":{},"能":{"docs":{},"通":{"docs":{},"关":{"docs":{},"了":{"docs":{},"，":{"docs":{},"通":{"docs":{},"关":{"docs":{},"了":{"docs":{},"说":{"docs":{},"明":{"docs":{},"不":{"docs":{},"了":{"docs":{},"太":{"docs":{},"多":{"docs":{},"问":{"docs":{},"题":{"docs":{},"。":{"docs":{"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"ref":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","tf":0.2}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"f":{"docs":{},"u":{"docs":{},"l":{"docs":{},"l":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"m":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{},"训":{"docs":{},"练":{"docs":{},"的":{"docs":{},"泛":{"docs":{},"化":{"docs":{},"不":{"docs":{},"到":{"docs":{},"r":{"docs":{},"e":{"docs":{},"l":{"docs":{},"u":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}}}}}}}}}}}}}},"(":{"docs":{},"x":{"docs":{},"_":{"docs":{},"i":{"docs":{},";":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.06896551724137931}}}}}}},"\\":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"t":{"docs":{},"a":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}},"f":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"{":{"1":{"docs":{},"}":{"docs":{},"{":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}},"docs":{}}}}}},"o":{"docs":{},"m":{"docs":{},"e":{"docs":{},"g":{"docs":{},"a":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}},")":{"docs":{},")":{"docs":{},")":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"v":{"docs":{},"a":{"docs":{},"r":{"docs":{},"(":{"docs":{},"δ":{"docs":{},"f":{"docs":{},"(":{"docs":{},"x":{"docs":{},"​":{"docs":{},"i":{"docs":{},"​":{"docs":{},"​":{"docs":{},";":{"docs":{},"ω":{"docs":{},")":{"docs":{},")":{"docs":{},")":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}},"​":{"docs":{},"f":{"docs":{},"​":{"docs":{},"^":{"docs":{},"​":{"docs":{},"​":{"docs":{},"(":{"docs":{},"x":{"docs":{},"​":{"docs":{},"i":{"docs":{},"​":{"docs":{},"​":{"docs":{},";":{"docs":{},"ω":{"docs":{},"+":{"docs":{},"δ":{"docs":{},"ω":{"docs":{},")":{"docs":{},")":{"docs":{},")":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"为":{"docs":{},"什":{"docs":{},"么":{"docs":{},"初":{"docs":{},"始":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"差":{"docs":{},"异":{"docs":{},"很":{"docs":{},"大":{"docs":{},"（":{"2":{"docs":{},"个":{"docs":{},"数":{"docs":{},"量":{"docs":{},"级":{"docs":{},"）":{"docs":{},"，":{"docs":{},"s":{"docs":{},"g":{"docs":{},"d":{"docs":{},"与":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"收":{"docs":{},"敛":{"docs":{},"速":{"docs":{},"度":{"docs":{},"却":{"docs":{},"差":{"docs":{},"不":{"docs":{},"多":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}},"收":{"docs":{},"敛":{"docs":{},"曲":{"docs":{},"线":{"docs":{},"没":{"docs":{},"有":{"docs":{},"画":{"docs":{},"出":{"docs":{},"收":{"docs":{},"敛":{"docs":{},"到":{"docs":{},"稳":{"docs":{},"态":{"docs":{},"的":{"docs":{},"整":{"docs":{},"个":{"docs":{},"过":{"docs":{},"程":{"docs":{},"，":{"docs":{},"最":{"docs":{},"终":{"docs":{},"的":{"docs":{},"收":{"docs":{},"敛":{"docs":{},"结":{"docs":{},"果":{"docs":{},"才":{"docs":{},"重":{"docs":{},"要":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"做":{"docs":{},"法":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}},"可":{"docs":{},"以":{"docs":{},"引":{"docs":{},"用":{"docs":{},"这":{"docs":{},"篇":{"docs":{},"文":{"docs":{},"章":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}},"回":{"docs":{},"报":{"docs":{},"设":{"docs":{},"计":{"docs":{},"：":{"docs":{},"r":{"docs":{},"=":{"docs":{},"−":{"1":{"docs":{},"s":{"docs":{},"−":{"1":{"docs":{},"∑":{"docs":{},"s":{"docs":{},"=":{"2":{"docs":{},"s":{"docs":{},"(":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"e":{"docs":{},"s":{"docs":{},")":{"docs":{},"−":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"(":{"docs":{},"e":{"docs":{},"s":{"docs":{},"−":{"1":{"docs":{},")":{"docs":{},")":{"docs":{},"r":{"docs":{},"=":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}}}}}},"引":{"docs":{},"用":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}},"收":{"docs":{},"敛":{"docs":{},"到":{"docs":{},"的":{"docs":{},"结":{"docs":{},"果":{"docs":{},"对":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"不":{"docs":{},"敏":{"docs":{},"感":{"docs":{},"，":{"docs":{},"而":{"docs":{},"r":{"docs":{},"m":{"docs":{},"s":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"p":{"docs":{},"与":{"docs":{},"s":{"docs":{},"g":{"docs":{},"d":{"docs":{},"都":{"docs":{},"对":{"docs":{},"初":{"docs":{},"始":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"敏":{"docs":{},"感":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"效":{"docs":{},"果":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}},"疑":{"docs":{},"点":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}},"目":{"docs":{},"的":{"docs":{},"：":{"docs":{},"为":{"docs":{},"了":{"docs":{},"避":{"docs":{},"免":{"docs":{},"初":{"docs":{},"始":{"docs":{},"学":{"docs":{},"习":{"docs":{},"率":{"docs":{},"依":{"docs":{},"赖":{"docs":{"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"ref":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","tf":0.034482758620689655}}}}}}}}}}}}}}}},"作":{"docs":{},"品":{"docs":{},"，":{"docs":{},"因":{"docs":{},"此":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}}},"应":{"docs":{},"该":{"docs":{},"不":{"docs":{},"错":{"docs":{},"。":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}}}}},"方":{"docs":{},"法":{"docs":{"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"ref":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","tf":0.08333333333333333}}}},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"d":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}},"不":{"docs":{},"错":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}},"每":{"docs":{},"维":{"docs":{},"采":{"docs":{},"取":{"docs":{},"共":{"docs":{},"享":{"docs":{},"的":{"docs":{},"策":{"docs":{},"略":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}}}}}},"输":{"docs":{},"入":{"docs":{},"是":{"docs":{},"梯":{"docs":{},"度":{"docs":{"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"ref":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","tf":0.045454545454545456}}}}}}},"库":{"docs":{},"，":{"docs":{},"可":{"docs":{},"以":{"docs":{},"进":{"docs":{},"行":{"docs":{},"参":{"docs":{},"考":{"docs":{},"。":{"docs":{"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285}}}}}}}}}}},"本":{"docs":{},"文":{"docs":{},"值":{"docs":{},"得":{"docs":{},"参":{"docs":{},"考":{"docs":{},"的":{"docs":{},"地":{"docs":{},"方":{"docs":{"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285}}}}}}}}}}},"里":{"docs":{},"面":{"docs":{},"使":{"docs":{},"用":{"docs":{},"了":{"docs":{},"一":{"docs":{},"个":{"docs":{"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"ref":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","tf":0.14285714285714285}}}}}}}}},"将":{"docs":{},"模":{"docs":{},"型":{"docs":{},"参":{"docs":{},"数":{"docs":{},"对":{"docs":{},"应":{"docs":{},"到":{"docs":{},"l":{"docs":{},"s":{"docs":{},"t":{"docs":{},"m":{"docs":{},"的":{"docs":{},"c":{"docs":{},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}}}}}}}}}}}}}},"解":{"docs":{},"决":{"docs":{},"的":{"docs":{},"问":{"docs":{},"题":{"docs":{},"：":{"docs":{},"f":{"docs":{},"e":{"docs":{},"w":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}}}}}}}}},"问":{"docs":{},"题":{"docs":{"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"ref":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","tf":0.058823529411764705}}}},"z":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}},"z":{"docs":{},"z":{"docs":{},",":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}},"π":{"docs":{},"θ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"t":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{},"z":{"docs":{},")":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"a":{"docs":{},"(":{"docs":{},"a":{"docs":{},"_":{"docs":{},"t":{"docs":{},"|":{"docs":{},"s":{"docs":{},"_":{"docs":{},"t":{"docs":{},",":{"docs":{},"z":{"docs":{},")":{"docs":{},"π":{"docs":{},"​":{"docs":{},"θ":{"docs":{},"​":{"docs":{},"​":{"docs":{},"(":{"docs":{},"a":{"docs":{},"​":{"docs":{},"t":{"docs":{},"​":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"​":{"docs":{},"t":{"docs":{},"​":{"docs":{},"​":{"docs":{},",":{"docs":{},"z":{"docs":{},")":{"docs":{"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"ref":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","tf":0.004032258064516129}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"length":434},"corpusTokens":["(x_i;","+","10","15","1e26","1e3","1e4","1}))r=−​s−1​​1​​​s=2​∑​s​​(log(e​s​​)−log(e​s−1​​))，其中s为总优化步数，回报实质上是衰减的误差变化量。","1}\\sum\\limits_{s=2}^s(log(e_s)","1}a​1:t−1​​","2","2001.icann.learn","2001.reinforc","2003.natur","2015.emnlp.languag","2016.aaai.learn","2016.neural","2016.nips.learn","2016.openai.rl$2$","2017.nips.optim","2019.nature.grandmast",">",">exploit",">leagu",">main",">oppo",">视觉空间",">语言区","\\delta","\\frac{1}{","\\omega","\\omega)))log(var(δf(x​i​​;ω)))","\\omega)))log(var(δ​f​^​​(x​i​​;ω+δω)))","a1:t−1a_{1:","acknowledg","act","action","actor","adam","advantag","agent","agent'","agent:","agents:","algorithm","algorithm(upgo)","algorithm:","alpahstar","apm","architectur","asynchron","attent","attr","auto","base","blog:","book","both","brain","build","build.sh","camera","cell","cell更是使用已经人为确定好的结构，把易于替代的ops（比如add,","cell的生成）就用了400块gpu，在6e+16的搜索空间中迭代了15000回。按照每次迭代1小时算，耗费了2个gpu年（除以400个gpu后实际时间耗费不到2天）。使用这么大的运算量，就算用遗传算法大概也能得到相当好的结果，很难说能不能算是强化学习的功劳，因为按照我的经验来说，很可能随机初始化的时候初始解就处在了一个相当不错的位置，挑最好的那个网络来说事随机性太强。","challeng","choic","ci.","clip","cname","cognit","cognition.md","combin","combinatori","comput","connected训练的泛化不到convolut","connections:","control","coordin","counter","critic","current","cyclic,","data","dataset","deep","depend","dependencies.md","descent","descent.md","differ","directories,","disobey","download","dqn，跑文字冒险类游戏。","effect","elem_mut）当作参数让网络预测，无法用于可变结构生成，灵活性基本没有。","env","episod","epub","estim","experi","exploit","explor","exploration.","f","fast","featur","few","fictiti","file","find","fix","follow","foundat","function","game","gitbook,","github","good","gool","gradient","gradient，","hard","hardli","header","human","ii","imit","imperfect","import","inform","init","integr","introduct","issu","item","iter","larg","latenc","layer","leagu","learn","learner","learning(td(λ\\lambdaλ))","learning.md","level","like","limit","list","locat","log(e_{","log(var(δf(xi;ω)))log(var(\\delta","log(var(δf^(xi;ω+δω)))log(var(\\delta\\hat","logic","long","lstm","lstm:","main","map","map(s,","markovian","mathmet","maze","meta","method","min","mind","mnist","mobi","model","monitor","move","move,","multi","nag","naiv","network","network:","neural","next","non","note","notes,","novel","o1:to_{1:","ob","object:","observ","offlin","onlin","oppo","oppon","opponents'","p","pages,","papernot","papers.pythonic.lif","paramet","partial","perspect","pfsp","play","play(pfsp)","play,","player'","pointer","polici","policies.","power","priortiz","problem","punish","quadrat","re","read","reading:","readme.md","real","realli","recurr","regress","reinforc","repali","reput","reviews.neur","reward","rl","rl的网络结构似乎只有一层","rmsprop","robust","sampling(v","scatter","search","select","self","sgd","share","shot","similar","size","slow","source:","space","space(1e3)","spacial:","sparse.","spatial","stack","starcraft","state","state，将学习率对应到lstm的输入，将梯度对应到lstm的candid","statist","step","strategi","strategy,","structur","subset","summary.md","superv","system","t","tactic","target","target,","task","tempor","term","text","this?","time","time:","tldr","to,","trace)","train","training.md","transit","travi","type(1e2):","typora.","t}o​1:t​​","understand","unit","us","valu","vast","version","via","view","weak","weights:","within","without","worker","world","write","z)","zzz,","·学习器：lstm","πθ(at∣st,z)\\pi_\\theta(a_t|s_t,z)π​θ​​(a​t​​∣s​t​​,z)","│","└──","├──","不错","为什么初始学习率差异很大（2个数量级），sgd与rmsprop收敛速度却差不多","为什么收敛曲线没有画出收敛到稳态的整个过程，最终的收敛结果才重要","了一个","代数","作品，reput","作品，因此","使用aur度量算法性能","使用rnn生成一个序列，实际上方法很简单。按顺序生成cnn的参数组合、shortcut节点等参数，然后stack一下就能生成cnn；至于自动生成lstm","使用了一种类似于trpo的强化学习算法：reps，限制新旧策略的kl散度。","使用了几个特征：","使用了梯度，在不同参数数目下的问题之间难以进行迁移。后来使用了share","使用元数据集，即多组数据集","使用强化学习与lstm结合，解决非马尔可夫问题或部分可观测马尔可夫决策过程（pomdps）","倒立摆","做法","元学习器：rl","可以引用这篇文章","回报设计：r=−1s−1∑s=2s(log(es)−log(es−1))r=","实验","将模型参数对应到lstm的cell","库，可以进行参考。","应该不错。","引用","推理","收敛到的结果对学习率不敏感，而rmsprop与sgd都对初始学习率敏感","效果","方法","最好的电子笔记本：gitbook","本文值得参考的地方","每维采取共享的策略","用full","用sigmoid训练的泛化不到relu","用一个游戏训练之后训练另一个游戏，能加快训练速度，实际上只有一个游戏的话语料也太少了吧，而且这个游戏的单词量也少得可怜；在一个文字冒险类游戏里面通关根本算不上是学会了什么或者能泛化之类的，因为完全靠记忆路径（文字冒险类游戏实际上是一个树结构的分支模型）就能通关了，通关了说明不了太多问题。","由于现有的研究都很浅，所以没有得到更详细的结论。说明推理和语言有关，代数和视觉有关，这在深度网络设计的时候有一定的参考价值（大概吧）。","疑点","目的：为了避免初始学习率依赖","结论","解决的问题：few","训练：当预测错了就惩罚，通过这种方式能让lstm收敛更快","评价","输入是梯度","这个实验只有google有实力做，因为光第二个试验（lstm","这篇文章可以用于在参考文献中提一下","这篇文章比较早（2001年发表），受限于当时的计算能力、调参技巧等，当时的实验都非常简单，没有太大参考意义；","这篇文章没有什么意思，用lstm","这篇是讲逻辑与数学认知的生理学基础，使用脑部扫描对逻辑和数学区域进行一些粗浅的认识。","这篇论文完全没有写出该方法的细节，只用了描述性的自然语言，所以无法进行复现，更难以进行评价。","里面使用了一个","问题"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"PaperNotes\nThis is my notes, powered by GitBook, GitHub Pages, Travis CI.\nBook Source: PaperNotes\nOnline Reading: papers.pythonic.life\n\nDownload Offline Version\n\nEPUB\nMOBI\n\nHow to Write Notes Likes This?\nYou can read my blog: 最好的电子笔记本：GitBook + Typora.\nFiles List\nNotes\n├── Meta-Learning\n│   ├── 2001.ICANN.Learning to Learning Using Gradient Descent.md\n│   ├── 2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.md\n│   ├── 2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.md\n│   ├── 2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.md\n│   ├── 2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.md\n│   ├── 2016.NIPS.Learning to learn by gradient descent by gradient descent.md\n│   ├── 2016.Neural Architecture Search with Reinforcement Learning.md\n│   ├── 2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.md\n│   └── 2017.NIPS.Optimization As a Model for Few-Shot Learning.md\n├── Reinforcement-Learning\n│   └── 2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.md\n├── CNAME\n├── HEADER\n├── README.md\n├── SUMMARY.md\n└── build.sh\n\n2 directories, 15 files\n"},"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html":{"url":"Meta-Learning/2001.ICANN.Learning to Learning Using Gradient Descent.html","title":"2001.ICANN.Learning to Learning Using Gradient Descent","keywords":"","body":"·学习器：LSTM\n元学习器：RL\n训练：当预测错了就惩罚，通过这种方式能让LSTM收敛更快\n"},"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html":{"url":"Meta-Learning/2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies.html","title":"2001.Reinforcement Learning with LSTM in Non-Markovian Tasks with Long-Term Dependencies","keywords":"","body":"使用强化学习与LSTM结合，解决非马尔可夫问题或部分可观测马尔可夫决策过程（POMDPs）\n实验\nT-maze\n倒立摆\n评价\n这篇文章比较早（2001年发表），受限于当时的计算能力、调参技巧等，当时的实验都非常简单，没有太大参考意义；\n这篇文章可以用于在参考文献中提一下\n"},"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html":{"url":"Meta-Learning/2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition.html","title":"2003.Nature Reviews.Neural Foundations of Logical and Mathmetical Cognition","keywords":"","body":"这篇是讲逻辑与数学认知的生理学基础，使用脑部扫描对逻辑和数学区域进行一些粗浅的认识。\n结论\n推理->语言区\n代数->视觉空间\n由于现有的研究都很浅，所以没有得到更详细的结论。说明推理和语言有关，代数和视觉有关，这在深度网络设计的时候有一定的参考价值（大概吧）。\n"},"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html":{"url":"Meta-Learning/2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning.html","title":"2015.EMNLP.Language Understanding for Text-based Games using Deep Reinforcement Learning","keywords":"","body":"这篇文章没有什么意思，用LSTM stack 了一个 DQN，跑文字冒险类游戏。\n用一个游戏训练之后训练另一个游戏，能加快训练速度，实际上只有一个游戏的话语料也太少了吧，而且这个游戏的单词量也少得可怜；在一个文字冒险类游戏里面通关根本算不上是学会了什么或者能泛化之类的，因为完全靠记忆路径（文字冒险类游戏实际上是一个树结构的分支模型）就能通关了，通关了说明不了太多问题。\n"},"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html":{"url":"Meta-Learning/2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training.html","title":"2016.AAAI.Learning Step Size Controllers for Robust Neural Network Training","keywords":"","body":"TLDR\n\n目的：为了避免初始学习率依赖\n\n做法\n\n使用了几个特征：\nlog(var(Δf^(xi;ω+Δω)))log(var(\\Delta\\hat f (x_i; \\omega + \\Delta \\omega)))log(var(Δ​f​^​​(x​i​​;ω+Δω)))\nlog(var(Δf(xi;ω)))log(var(\\Delta f (x_i; \\omega)))log(var(Δf(x​i​​;ω)))\n\n\n使用了一种类似于TRPO的强化学习算法：REPS，限制新旧策略的KL散度。\n回报设计：r=−1S−1∑s=2S(log(Es)−log(Es−1))r=-\\frac{1}{S-1}\\sum\\limits_{s=2}^S(log(E_s) - log(E_{s-1}))r=−​S−1​​1​​​s=2​∑​S​​(log(E​s​​)−log(E​s−1​​))，其中S为总优化步数，回报实质上是衰减的误差变化量。\nRL的网络结构似乎只有一层\n\n效果\n收敛到的结果对学习率不敏感，而RMSProp与SGD都对初始学习率敏感\n疑点\n\n为什么初始学习率差异很大（2个数量级），SGD与RMSProp收敛速度却差不多\n为什么收敛曲线没有画出收敛到稳态的整个过程，最终的收敛结果才重要\n\n引用\n可以引用这篇文章\n"},"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html":{"url":"Meta-Learning/2016.Neural Architecture Search with Reinforcement Learning.html","title":"2016.Neural Architecture Search with Reinforcement Learning","keywords":"","body":"Goole Brain 作品，因此 reputation 应该不错。\n方法\n使用RNN生成一个序列，实际上方法很简单。按顺序生成CNN的参数组合、shortcut节点等参数，然后stack一下就能生成CNN；至于自动生成LSTM CELL更是使用已经人为确定好的结构，把易于替代的ops（比如add, elem_mut）当作参数让网络预测，无法用于可变结构生成，灵活性基本没有。\n评价\n这个实验只有Google有实力做，因为光第二个试验（LSTM CELL的生成）就用了400块GPU，在6e+16的搜索空间中迭代了15000回。按照每次迭代1小时算，耗费了2个GPU年（除以400个GPU后实际时间耗费不到2天）。使用这么大的运算量，就算用遗传算法大概也能得到相当好的结果，很难说能不能算是强化学习的功劳，因为按照我的经验来说，很可能随机初始化的时候初始解就处在了一个相当不错的位置，挑最好的那个网络来说事随机性太强。\n"},"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html":{"url":"Meta-Learning/2016.NIPS.Learning to learn by gradient descent by gradient descent.html","title":"2016.NIPS.Learning to learn by gradient descent by gradient descent","keywords":"","body":"Deep Mind 作品，Reputation 不错\nMethod\n\n每维采取共享的策略\n输入是梯度\n使用AUR度量算法性能\n\nExperiment\nAlgorithm\n\nADAM\nRMSprop\nSGD\nNAG\n\nDatasets\n\nQuadratics\nMNIST\n\nProblems\n\n用sigmoid训练的泛化不到relu\n用full connected训练的泛化不到convolution layer\n\n"},"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html":{"url":"Meta-Learning/2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning.html","title":"2016.OpenAI.RL$2$ Fast Reinforcement Learning via Slow Reinforcement Learning","keywords":"","body":"TLDR\n这篇论文完全没有写出该方法的细节，只用了描述性的自然语言，所以无法进行复现，更难以进行评价。\n本文值得参考的地方\nAcknowledgements 里面使用了一个 RL 库，可以进行参考。\n"},"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html":{"url":"Meta-Learning/2017.NIPS.Optimization As a Model for Few-Shot Learning.html","title":"2017.NIPS.Optimization As a Model for Few-Shot Learning","keywords":"","body":"TLDR\n\nmeta-learning\n将模型参数对应到LSTM的cell state，将学习率对应到LSTM的输入，将梯度对应到LSTM的candidate cell state\n使用元数据集，即多组数据集\n\n解决的问题：Few-Shot Learning\n问题\n\n使用了梯度，在不同参数数目下的问题之间难以进行迁移。后来使用了share parameters across the coordinates of the learner gradient，\n\n"},"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html":{"url":"Reinforcement-Learning/2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning.html","title":"2019.Nature.Grandmaster level in StarCraft II using multi-agent reinforcement learning","keywords":"","body":"AlpahStar\nChallenges\n\nvast space of cyclic, non-transitive strategies and counter-strategies\nself-play hard to get novel strategies\nstrategies not effective in real world play\nlarge action space(1e3)\nimperfect information\n\nHuman Data\nHardly explore useful tactics with naive exploration. Good actions are really sparse.\n\nsupervized learning\nhuman statistic zzz, reward agents which follows human strategy, or punish actions disobeying supervizing policies.\n\nENV\n\n1e4 steps\n10-min episode\n\nOBS\n\na list of observable items and its attrs\n\nAct\n\n1e3 choices\nstructured\naction type(1e2): move, build a worker\nobject: who to issue that action to, for any subset of the agent's units\nspacial: where to target, among locations on the map or units within the camera view\ntime: when the next action will be issued\n\n\nmove camera view\n1e26 combinations\n\nStates\n\no1:to_{1: t}o​1:t​​\na1:t−1a_{1: t-1}a​1:t−1​​\n\nMonitor layer\n\nnetwork latency and computation time\nAPM limits\n\nPolicy\nπθ(at∣st,z)\\pi_\\theta(a_t|s_t,z)π​θ​​(a​t​​∣s​t​​,z)\nmap(s, z) -> a\nAgents\n\nAlgorithm: \npolicy gradient similar to advantage actor-critic\nasynchronously on repalyed experiences\noff-policy\n\n\ntemporal difference learning(TD(λ\\lambdaλ))\nclipped importance sampling(V-trace)\nself-imitation algorithm(UPGO)\nvalue function share weights: estimated using both player's and opponents's perspectives\n\nLeague training\nPriortized Fictitious self-play(PFSP)\n\nmain agent: priortized fictitious self-play, be selected as an opponent\nmain exploiter agents: play with only current iteration of main agent\n\n\n\n\n\n\n\nagents\n\n\n\nmain\n\nmain\n\n\n\noppo\n\noppo\n\noppo -->\n\nmain->oppo\n\n\nfixed p\n\nmain -->\n\noppo->main\n\n\n\n\n\nexploit\n\nexploit\n\nmain -->\n\nexploit->main\n\n\ncurrent main iteration\nfind weakness\n\nexploit -->\n\nexploit->exploit\n\n\nre-init\n\n\n\nleague\n\nleague exploiter agents\n\nmain -->\n\nleague->main\n\n\nPFSP without targeted by main exploiter agents\nfinds system weakness\n\nleague -->\n\nleague->league\n\n\nre-init\n\n\n\n\nModel\n\nself-attention of obs\nscatter connections: to integrate spatial and non-spatial features\nLSTM: partial obs\nauto-regressive policy and recurrent pointer network: structured combinatorial action space\n\n"}}}