## TLDR

- 目的：为了避免初始学习率依赖

# 做法

- 使用了几个特征：
  1. $$log(var(\Delta\hat f (x_i; \omega + \Delta \omega)))$$
  2. $$log(var(\Delta f (x_i; \omega)))$$
- 使用了一种类似于TRPO的强化学习算法：REPS，限制新旧策略的KL散度。
- 回报设计：$$r=-\frac{1}{S-1}\sum\limits_{s=2}^S(log(E_s) - log(E_{s-1}))$$，其中S为总优化步数，回报实质上是衰减的误差变化量。
- RL的网络结构似乎只有一层

# 效果

收敛到的结果对学习率不敏感，而RMSProp与SGD都对初始学习率敏感

# 疑点

1. 为什么初始学习率差异很大（2个数量级），SGD与RMSProp收敛速度却差不多
2. 为什么收敛曲线没有画出收敛到稳态的整个过程，最终的收敛结果才重要

# 引用

可以引用这篇文章qqq